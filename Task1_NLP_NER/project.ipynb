{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing. Named entity recognition\n",
    "\n",
    "### **description:** This file is demo of project with an explanation/instruction on the project logic and the procedure for interacting with files and their task/content. \n",
    "\n",
    "### author: Bytsenko Anna\n",
    "date: 13.11.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Run data generation.ipynb\n",
    "\n",
    "This notebook with explanation **prepares a dataset for a Named Entity Recognition (NER)** task by generating, annotating, and processing data related to mountain names. \n",
    "\n",
    "*It uses a combination of data scraping, annotation, and formatting steps.*\n",
    "\n",
    "    1. Data Generation\n",
    "\n",
    "`Description`: This section involves generating sentences about mountains and annotating them with BIO (Begin, Inside, Outside) labels to mark named entities.\n",
    "\n",
    "    1.1 Mountain Name Scraping\n",
    "\n",
    "Code Functionality:\n",
    "Scrapes a webpage containing a list of mountains (in this case, from Britannica).\n",
    "Extracts mountain names from specific HTML elements using the BeautifulSoup library.\n",
    "Ensures that only unique mountain names are retained.\n",
    "\n",
    "    1.2 Data Cleaning and Annotation\n",
    "\n",
    "This step likely involves processing the scraped names and generating labeled datasets, though further analysis of the notebook is needed to confirm.\n",
    "\n",
    "    2. Data Annotation\n",
    "\n",
    "`Description`: This section annotates sentences with BIO (Begin-Inside-Outside) labels for mountains' names.\n",
    "\n",
    "    2.1 Random Sentence Generation\n",
    "\n",
    "`Description`: Random sentences are generated, embedding mountain names into various contexts. This creates examples where mountain names are either present or absent in natural-sounding sentences.\n",
    "\n",
    "Key Steps:\n",
    "\n",
    "- Define templates with placeholders for mountain names.\n",
    "- Randomly select mountain names and fill placeholders.\n",
    "- Store annotated sentences and corresponding labels.\n",
    "\n",
    "    3. Test Dataset Preparation\n",
    "\n",
    "Similar to the previous training data generation, combining the two points to create a test data set using identical methods and order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run train_model.py\n",
    "\n",
    "`Description`: This code is part of a machine learning pipeline for training a Named Entity Recognition (NER) model to identify mountain names in text. \n",
    "\n",
    "The primary steps include:\n",
    "\n",
    "- Loading and processing the annotated dataset containing sentences and their corresponding labels (e.g., B-MOUNTAIN, I-MOUNTAIN, O).\n",
    "\n",
    "- Preparing data for tokenization and aligning labels with tokenized words.\n",
    "\n",
    "- Initializing a pretrained BERT model for token classification.\n",
    "\n",
    "- Splitting the data into training and testing sets.\n",
    "\n",
    "- Training the model on the training set and evaluating it on the test set.\n",
    "\n",
    "- Saving the trained model and tokenizer for future use.\n",
    "\n",
    "- Generating evaluation metrics (e.g., classification report) to assess the model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run inference.py \n",
    "\n",
    "`Description`: This code demonstrates the use of a trained BERT-based Named Entity Recognition (NER) model to identify and classify mountain names in given text inputs. It includes steps to load a pre-trained model, tokenize input sentences, make predictions, and handle subtokens to produce a coherent output. \n",
    "\n",
    "The key goals are to:\n",
    "\n",
    "- Tokenize input text: Split the text into tokens that the model understands.\n",
    "\n",
    "- Predict entity labels: Use the trained model to assign labels (e.g., B-MOUNTAIN, I-MOUNTAIN, O) to tokens.\n",
    "\n",
    "- Post-process tokens: Merge subtokens and remove special tokens for a human-readable output.\n",
    "\n",
    "- Display results: Present predictions for each token in the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional step: Run evaluate_model.py\n",
    "\n",
    "`Description`: This code evaluates a pre-trained Named Entity Recognition (NER) model for identifying mountain names in a given text. \n",
    "\n",
    "It performs the following steps:\n",
    "\n",
    "- Loads a saved model and tokenizer.\n",
    "\n",
    "- Processes test data, aligning labels with tokenized inputs.\n",
    "\n",
    "- Prepares the data in a format suitable for evaluation.\n",
    "\n",
    "- Uses the Trainer class from the Hugging Face library to predict and evaluate the model's performance on the test dataset.\n",
    "\n",
    "- Generates a classification report and calculates additional evaluation metrics (precision, recall, and F1 score).\n",
    "\n",
    "- Result of model performance's analysis(detailed info at the end of evaluate_model.py file):\n",
    "\n",
    "\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **High Recall Across All Classes**  \n",
    "   The recall values of 1.00 for both `B-MOUNTAIN` and `I-MOUNTAIN` indicate that the model successfully identifies all true mountain-related entities in the dataset.  \n",
    "\n",
    "2. **Slightly Lower Precision for `B-MOUNTAIN`**  \n",
    "   The precision of 0.98 for `B-MOUNTAIN` suggests a small number of false positives. These could be due to non-mountain tokens being mistakenly classified as `B-MOUNTAIN`.  \n",
    "\n",
    "3. **Perfect Performance for `I-MOUNTAIN`**  \n",
    "   Both precision and recall for `I-MOUNTAIN` are 1.00, showing that the model flawlessly identifies continuation tokens.  \n",
    "\n",
    "4. **Overall Weighted Metrics**  \n",
    "   - Weighted Precision: **0.9903**  \n",
    "     Indicates that, on average, 99% of all predicted entities (weighted by class size) are correct.  \n",
    "   - Weighted Recall: **1.0000**  \n",
    "     All true entities in the dataset are captured by the model.  \n",
    "   - Weighted F1-Score: **0.9951**  \n",
    "     Combines high precision and perfect recall, confirming robust model performance. \n",
    "\n",
    " \n",
    "*High/close to ideal values of the model's evaluation characteristics may be triggered by the nature of the data (synthetically generated by templates)*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
